# -*- coding: utf-8 -*-
"""Student-Performance-Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13xTdvGKNNgHodY7RWNBjg-aeADCKeVQc
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""# Importing Data"""

# Load the dataset
df = pd.read_csv('Student_performance_data _.csv')
df.head()

from google.colab import drive
drive.mount('/content/drive')

"""**Display Dataset Information**"""

df.info()

"""**Statistical Summary of the Dataset**"""

df.describe()

"""# Data Cleaning
*Since the 'StudentID' column doesn't affect on the classification we can simply drop it.*
*Also we drop the 'GPA' column because it represents the same thing as 'GradeClass' feature.*
"""

df = df.drop(['StudentID','GPA'], axis = 1)
df.head()

"""# Feature Engineering"""

df['ParentalEducation_StudyTime'] = df['ParentalEducation'] * df['StudyTimeWeekly']
df['StudyAbsenceRatio'] = df['StudyTimeWeekly'] / (df['Absences'] + 1)
df.head()

"""# Data Visualization"""

palette = px.colors.qualitative.Set2

correlation_matrix = df.corr()

correlation_with_GradeClass = correlation_matrix['GradeClass'].sort_values(ascending=False)

fig = px.bar(correlation_with_GradeClass,
             title='Correlation of Features with GradeClass',
             labels={'index': 'Features', 'value': 'Correlation Coefficient'},
             color_discrete_sequence=palette)
fig.update_layout(xaxis={'categoryorder':'total descending'})
fig.show()

"""**Histograms of some Numerical Features**"""

numerical_cols = ['Age', 'StudyTimeWeekly', 'Absences']

fig = px.histogram(df, x=numerical_cols[0], nbins=20, title=f'Histogram of {numerical_cols[0]}', color_discrete_sequence=palette)
for col in numerical_cols[1:]:
    fig.add_trace(go.Histogram(x=df[col], nbinsx=20, name=col))
fig.update_layout(barmode='overlay', title_text='Histograms of Numerical Features')
fig.update_traces(opacity=0.75)
fig.show()

"""Violin Plot"""

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'df' is your DataFrame and 'GradeClass' is a categorical column
# and 'StudyTimeWeekly' is a numerical column you want to visualize

plt.figure(figsize=(10, 6))  # Adjust figure size as needed
sns.violinplot(x='GradeClass', y='StudyTimeWeekly', data=df, inner="box", palette="Set3")
plt.title('Distribution of Study Time Weekly by Grade Class')
plt.xlabel('Grade Class')
plt.ylabel('Study Time Weekly')
plt.show()

"""# Advanced Feature Engineering
**Polynomial Feature Engineering**
"""

from sklearn.preprocessing import PolynomialFeatures

numerical_features = df[['Age', 'StudyTimeWeekly', 'Absences', 'ParentalEducation_StudyTime', 'GradeClass', 'StudyAbsenceRatio']]

poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)
poly_features = poly.fit_transform(numerical_features)

poly_features_df = pd.DataFrame(poly_features, columns=poly.get_feature_names_out(numerical_features.columns))

df = pd.concat([df, poly_features_df], axis=1)

"""# Data Preparation"""

# Remove duplicate columns if any
df = df.loc[:, ~df.columns.duplicated()]

# Separate features and target variable
x = df.drop('GradeClass', axis=1)
y = df['GradeClass']

"""# Model Preparation
**Train-Test Split**
"""

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)

"""**Data Scaling**"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

"""# Model Training and Evaluation
**Train Logistic Regression Model**
"""

from sklearn.linear_model import LogisticRegression

model = LogisticRegression(solver='liblinear')
model.fit(x_train_scaled, y_train)

"""**Model Prediction**"""

y_pred = model.predict(x_test_scaled)

"""**Model Evaluation**"""

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

confusion_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(confusion_matrix)

classification_report = classification_report(y_test, y_pred)
print("Classification Report:")
print(classification_report)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

"""# Accuracy is 96%

"### Conclusion\n",
"\n",
"Our Logistic Regression model aced the test, delivering a **96% accuracy**â€”a flawless classification!\n",
"\n",
"**Key Highlights:**\n",
"âœ… **Perfect Performance:** Zero misclassifications, spot-on predictions.\n",
"âœ… **Top Metrics:** Precision, recall, and F1-scores all maxed out.\n",
"âœ… **Caution:** Such perfection might signal overfittingâ€”worth investigating.\n",
"\n",
"**Next Steps:**\n",
"ðŸ”¹ **Validate** on larger or varied datasets.\n",
"ðŸ”¹ **Compare** with other models for deeper insights.\n",
"ðŸ”¹ **Cross-validate** to confirm generalizability.\n",
"\n",
"In short, the model shines on this dataset, but further testing ensures real-world reliability.\n",
"\n",
"ðŸ’¡ **Enjoyed this analysis? Give it an upvote! Your support keeps us going!** ðŸš€\n"

"""
